{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA topic model for content for content categories in security policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk==3.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvisualize\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove punctuation and special characters\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best number of topics based on coherence and perplexity scores\n",
    "data = pd.read_csv('dataset/Document_All_Category.csv')\n",
    "\n",
    "data['processed_Document'] = data['Document'].apply(preprocess_text)\n",
    "texts = data['processed_Document'].tolist()\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=2)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "num_topics_list = range(2, 20)\n",
    "coherence_scores = []\n",
    "perplexity_scores = []\n",
    "\n",
    "# Train models and calculate coherence and perplexity scores\n",
    "for num_topics in num_topics_list:\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, random_state=63,\n",
    "                         chunksize=20, num_topics=num_topics, passes=100, iterations=200, eta=0.5)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores.append(coherence_model.get_coherence())\n",
    "\n",
    "    # Compute Perplexity Score\n",
    "    perplexity_scores.append(lda_model.log_perplexity(corpus))\n",
    "\n",
    "# Plot the coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_topics_list, coherence_scores, marker='o', label='Coherence Score', color='b')\n",
    "plt.xlabel('# Topics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Coherence Score vs. Number of Topics')\n",
    "plt.xticks(num_topics_list)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the perplexity scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_topics_list, perplexity_scores, marker='o', label='Perplexity Score', color='r')\n",
    "plt.xlabel('# Topics')\n",
    "plt.ylabel('Log Perplexity')\n",
    "plt.title('Perplexity Score vs. Number of Topics')\n",
    "plt.xticks(num_topics_list)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA modeling for all documents\n",
    "data = pd.read_csv('dataset/Document_All_Category.csv')\n",
    "\n",
    "data['processed_Document'] = data['Document'].apply(preprocess_text)\n",
    "texts = data['processed_Document'].tolist()\n",
    "\n",
    "# load dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below = 2)\n",
    "\n",
    "# words to be procedd\n",
    "total_words = sum(len(text) for text in texts)\n",
    "print(\"Total words after preprocessing:\", total_words)\n",
    "unique_words = len(dictionary)\n",
    "print(\"Unique words after filtering:\", unique_words)\n",
    "\n",
    "\n",
    "# generate corpus as BoW\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# train LDA model\n",
    "num_topics=6\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, chunksize=20, num_topics=num_topics, passes=100, iterations=200, random_state=63, eta=0.5)\n",
    "\n",
    "for topic in lda_model.print_topics(num_topics=num_topics, num_words=10):\n",
    "    print(topic)\n",
    "\n",
    "coherence_model = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print('Coherence:', coherence_score)\n",
    "\n",
    "perplexity_score = lda_model.log_perplexity(corpus)\n",
    "print('Perplexity:', perplexity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topics visualization\n",
    "# Get topic distributions for each document\n",
    "topic_distributions = lda_model.get_document_topics(corpus, minimum_probability=0)\n",
    "\n",
    "# Convert topic distributions to a matrix\n",
    "topic_matrix = np.zeros((len(topic_distributions), num_topics))\n",
    "for i, dist in enumerate(topic_distributions):\n",
    "    for topic_num, prob in dist:\n",
    "        topic_matrix[i, topic_num] = prob\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne_model = TSNE(n_components=2, random_state=42)\n",
    "tsne_values = tsne_model.fit_transform(topic_matrix)\n",
    "\n",
    "# Get the dominant topic for each document\n",
    "dominant_topics = np.argmax(topic_matrix, axis=1)\n",
    "\n",
    "# Create a DataFrame to store t-SNE values and dominant topics\n",
    "tsne_df = pd.DataFrame(tsne_values, columns=['x', 'y'])\n",
    "tsne_df['topic'] = dominant_topics\n",
    "\n",
    "# Plot the t-SNE results with topic labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(tsne_df['x'], tsne_df['y'], c=tsne_df['topic'], cmap='viridis', s=50, alpha=0.7)\n",
    "plt.colorbar(label='Topic')\n",
    "plt.title('t-SNE Clustering of LDA Topics')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the topics and their corresponding colors\n",
    "for topic in range(num_topics):\n",
    "    terms = [term for term, _ in lda_model.show_topic(topic, topn=10)]\n",
    "    print(f\"Topic {topic}: \" + \", \".join(terms) + f\" (Color: {scatter.cmap(scatter.norm(topic))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 most relevance terms in each topics\n",
    "# visualization\n",
    "dickens_visual = gensimvisualize.prepare(lda_model, corpus, dictionary, mds='mmds')\n",
    "# pyLDAvis.display(dickens_visual)\n",
    "\n",
    "topic_info = dickens_visual.topic_info\n",
    "topic_terms = topic_info.groupby('Category')\n",
    "\n",
    "category_term_counts = {topic: Counter() for topic in topic_terms.groups}\n",
    "category_term_details = {topic: {} for topic in topic_terms.groups}\n",
    "\n",
    "for topic, group in topic_terms:\n",
    "    print(f\"Topic {topic}:\")\n",
    "    sorted_words = group.sort_values(by='Freq', ascending=False)\n",
    "    top_words = sorted_words[['Term', 'Freq']].head(10)\n",
    "    for word, freq in zip(top_words['Term'], top_words['Freq']):\n",
    "        print(f\"  {word} ({freq:.2f})\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "top_terms_per_topic = {}\n",
    "\n",
    "for topic, group in topic_terms:\n",
    "    sorted_words = group.sort_values(by='Freq', ascending=False)\n",
    "    top_terms = sorted_words[['Term', 'Freq']].head(10)\n",
    "    top_terms_per_topic[topic] = set(top_terms['Term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count terms occurance in each category\n",
    "data = pd.read_csv('dataset/Document_4_Category.csv')\n",
    "data['processed_Document'] = data['Document'].apply(preprocess_text)\n",
    "\n",
    "topic_terms = lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False)\n",
    "top_terms_per_topic = {topic: [term for term, _ in words] for topic, words in topic_terms}\n",
    "\n",
    "# count term occurrences in each category\n",
    "category_term_counts = defaultdict(Counter)\n",
    "category_term_details = defaultdict(lambda: defaultdict(Counter))\n",
    "\n",
    "# Identify terms that appear in multiple topics\n",
    "term_topic_mapping = defaultdict(set)\n",
    "for topic, terms in top_terms_per_topic.items():\n",
    "    for term in terms:\n",
    "        term_topic_mapping[term].add(topic)\n",
    "multi_topic_terms = {term: topics for term, topics in term_topic_mapping.items() if len(topics) > 1}\n",
    "\n",
    "print(\"Multi-topic terms:\")\n",
    "for term, topics in multi_topic_terms.items():\n",
    "    print(f\"{term}: Topics {sorted(topics)}\")\n",
    "\n",
    "multi_topic_term_category_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Count occurrences of multi-topic terms per category\n",
    "for term, topics in multi_topic_terms.items():\n",
    "    for _, row in data.iterrows():\n",
    "        category = row['Category']\n",
    "        document_content = str(row['processed_Document']).lower()\n",
    "        count = document_content.count(term)\n",
    "\n",
    "        if count > 0:\n",
    "            multi_topic_term_category_counts[term][category] += count\n",
    "\n",
    "# Count occurrences only for non-multi-topic terms\n",
    "for topic, top_terms in top_terms_per_topic.items():\n",
    "    single_topic = [term for term in top_terms if term not in multi_topic_terms]\n",
    "    for _, row in data.iterrows():\n",
    "        category = row['Category']\n",
    "        document_content = str(row['processed_Document']).lower()\n",
    "\n",
    "        term_counts = {term: document_content.count(term) for term in single_topic}\n",
    "        total_count = sum(term_counts.values())\n",
    "\n",
    "        category_term_counts[topic][category] += total_count\n",
    "\n",
    "        if category not in category_term_details[topic]:\n",
    "            category_term_details[topic][category] = Counter()\n",
    "        category_term_details[topic][category].update(term_counts)\n",
    "\n",
    "\n",
    "multi_topic_term_df = pd.DataFrame(multi_topic_term_category_counts).T.fillna(0)\n",
    "multi_topic_term_df['Topics'] = multi_topic_term_df.index.map(lambda term: sorted(multi_topic_terms[term]))\n",
    "\n",
    "# most occurrences for each topic\n",
    "topic_category_analysis = {\n",
    "    topic: {\n",
    "        \"Category\": category_counts.most_common(1)[0][0] if category_counts else \"None\",\n",
    "        \"Occurrences\": category_counts.most_common(1)[0][1] if category_counts else 0,\n",
    "        \"Terms\": category_term_details[topic][category_counts.most_common(1)[0][0]] if category_counts else {}\n",
    "    }\n",
    "    for topic, category_counts in category_term_counts.items()\n",
    "}\n",
    "\n",
    "print(multi_topic_terms)\n",
    "display(multi_topic_term_df)\n",
    "display(topic_category_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
